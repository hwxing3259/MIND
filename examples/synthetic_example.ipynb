{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5214cd63-8983-4bfe-9f1f-7ef884ea94be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/work_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "import torch.multiprocessing\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.module import Module\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import pairwise_distances, adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snf\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from sklearn.cluster import spectral_clustering, KMeans\n",
    "from sklearn.metrics import v_measure_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from MIND import MIND"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b8f89-3272-4ea8-8736-fa85f8297d9f",
   "metadata": {},
   "source": [
    "# Synthetic data with high noise\n",
    "Here we demonstrate the training/testing process of the proposed method using the high noise data. Training/testing of the low noise version is identical. \n",
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14a19ac5-2aae-44a2-96d4-bbb86841b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify hyperparameters\n",
    "emb_dim = 64\n",
    "lr = 1e-4\n",
    "epoch = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4322e2f2-4984-498c-8e4c-817145b4a805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "Epoch=0\n",
      "Epoch=1000\n",
      "Epoch=2000\n",
      "Epoch=3000\n",
      "Epoch=4000\n",
      "0.2\n",
      "Epoch=0\n",
      "Epoch=1000\n",
      "Epoch=2000\n",
      "Epoch=3000\n",
      "Epoch=4000\n",
      "0.3\n",
      "Epoch=0\n",
      "Epoch=1000\n",
      "Epoch=2000\n",
      "Epoch=3000\n",
      "Epoch=4000\n",
      "0.4\n",
      "Epoch=0\n",
      "Epoch=1000\n",
      "Epoch=2000\n",
      "Epoch=3000\n",
      "Epoch=4000\n",
      "0.5\n",
      "Epoch=0\n",
      "Epoch=1000\n",
      "Epoch=2000\n",
      "Epoch=3000\n",
      "Epoch=4000\n",
      "0.6\n",
      "Epoch=0\n",
      "Epoch=1000\n",
      "Epoch=2000\n",
      "Epoch=3000\n",
      "Epoch=4000\n",
      "0.7\n",
      "Epoch=0\n",
      "Epoch=1000\n",
      "Epoch=2000\n",
      "Epoch=3000\n",
      "Epoch=4000\n",
      "0.8\n",
      "Epoch=0\n",
      "Epoch=1000\n",
      "Epoch=2000\n",
      "Epoch=3000\n",
      "Epoch=4000\n",
      "0.9\n",
      "Epoch=0\n",
      "Epoch=1000\n",
      "Epoch=2000\n",
      "Epoch=3000\n",
      "Epoch=4000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(31415)\n",
    "torch.manual_seed(31415)\n",
    "for i, frac in enumerate((1+np.arange(9))/10):\n",
    "    print(frac)\n",
    "    sim_3 = pd.read_csv('./synthetic_data/sim_methyl_high.csv', index_col=0)\n",
    "    sim_2 = pd.read_csv('./synthetic_data/sim_protein_high.csv', index_col=0)\n",
    "    sim_1 = pd.read_csv('./synthetic_data/sim_expr_high.csv', index_col=0)\n",
    "    sim_cls = pd.read_csv('./synthetic_data/sim_cls_high.csv', index_col=1)\n",
    "\n",
    "    non_shared = list(np.random.choice(range(sim_1.shape[0]), size=int((1 - frac) * sim_1.shape[0]), replace=False))\n",
    "    shared = list(set(range(sim_1.shape[0])) - set(non_shared))\n",
    "    sim_1_presence = torch.tensor([_ in shared + non_shared[:len(non_shared) // 3] for _ in range(sim_1.shape[0])])\n",
    "    sim_2_presence = torch.tensor(\n",
    "        [_ in shared + non_shared[len(non_shared) // 3:(2 * len(non_shared) // 3)] for _ in range(sim_2.shape[0])])\n",
    "    sim_3_presence = torch.tensor([_ in shared + non_shared[(2 * len(non_shared) // 3):] for _ in range(sim_3.shape[0])])\n",
    "\n",
    "    sim_1 = torch.tensor(sim_1.to_numpy(), dtype=torch.float)\n",
    "    sim_2 = torch.tensor(sim_2.to_numpy(), dtype=torch.float)\n",
    "    sim_3 = torch.tensor(sim_3.to_numpy(), dtype=torch.float)\n",
    "    sim_3 = torch.log(sim_3 / (1 - sim_3))\n",
    "\n",
    "    sim_1_test = sim_1[~sim_1_presence] * 1.0\n",
    "    sim_2_test = sim_2[~sim_2_presence] * 1.0\n",
    "    sim_3_test = sim_3[~sim_3_presence] * 1.0\n",
    "    test_list = [sim_1_test.to(device), sim_2_test.to(device), sim_3_test.to(device)]\n",
    "    \n",
    "    sim_1[~sim_1_presence] = float('nan')\n",
    "    sim_2[~sim_2_presence] = float('nan')\n",
    "    sim_3[~sim_3_presence] = float('nan')\n",
    "\n",
    "    data_dict = {'RNA_expr': pd.DataFrame(sim_1.cpu().numpy()), \n",
    "                 'Protein': pd.DataFrame(sim_2.cpu().numpy()),\n",
    "                 'DNA_methyl': pd.DataFrame(sim_3.cpu().numpy())}\n",
    "\n",
    "    test = MIND(data_dict=data_dict, device=device, emb_dim=emb_dim).to(device)\n",
    "    test.my_train(epoch, lr=lr)\n",
    "    with torch.no_grad():\n",
    "        z = test.get_embedding()[0].cpu().numpy()\n",
    "    np.savetxt('./synth_results/embeddings_frac_{}.txt'.format(frac), z)\n",
    "\n",
    "    reconstructed = test.predict()\n",
    "    masks = [sim_1_presence, sim_2_presence, sim_3_presence]\n",
    "    names = ['RNA_expr', 'Protein', 'DNA_methyl']\n",
    "    for _ in range(len(reconstructed)):\n",
    "        pred = reconstructed[_][~masks[_]].cpu().numpy()\n",
    "        obs = test_list[_].cpu().numpy()\n",
    "        np.savetxt('./synth_results/{}_missing_obs_high_noise_frac_{}.txt'.format(names[_], frac), obs.ravel())\n",
    "        np.savetxt('./synth_results/{}_missing_pred_high_noise_frac_{}.txt'.format(names[_], frac), pred.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2c96a9-3214-49e4-9752-9486429cbcb2",
   "metadata": {},
   "source": [
    "## Downstreaming task 1: clustering\n",
    "Apply spectral clustering to the output embedddings. Compare them with the true membership using normalised mutual information as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "943c8ae5-5229-49f7-9b48-52107ee16cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          NMI\n",
      "0.1  0.589354\n",
      "0.2  0.730374\n",
      "0.3  0.787495\n",
      "0.4  0.884238\n",
      "0.5  0.902163\n",
      "0.6  0.932714\n",
      "0.7  0.963517\n",
      "0.8  0.984615\n",
      "0.9  0.996303\n"
     ]
    }
   ],
   "source": [
    "cluster_number = 15  # the true number of clusters\n",
    "my_res_NMI = pd.DataFrame(0., index=(1+np.arange(9))/10, columns=['NMI'])\n",
    "for i, frac in enumerate((1+np.arange(9))/10):\n",
    "    sim_cls = pd.read_csv('./synthetic_data/sim_cls_high.csv', index_col=1)\n",
    "    z = np.loadtxt('./synth_results/embeddings_frac_{}.txt'.format(frac))\n",
    "    temp = 0.\n",
    "    for _ in range(10):\n",
    "        labels = spectral_clustering(pairwise_kernels(z, metric='rbf'), n_clusters=cluster_number)\n",
    "        temp += v_measure_score(sim_cls['cluster.id'].to_numpy(), labels)\n",
    "    my_res_NMI.loc[frac, 'NMI'] = temp/10.\n",
    "print(my_res_NMI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75ad6c1-8c30-437b-8968-7096a702f97e",
   "metadata": {},
   "source": [
    "## Downstreaming task 2: Classification\n",
    "Fit XGBoost classifiers to predict membership using the output embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "204bc427-350d-46d0-beba-51286a60ff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Accuracy\n",
      "0.1     0.786\n",
      "0.2     0.860\n",
      "0.3     0.890\n",
      "0.4     0.948\n",
      "0.5     0.946\n",
      "0.6     0.952\n",
      "0.7     0.964\n",
      "0.8     0.964\n",
      "0.9     0.976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "my_res_ACC = pd.DataFrame(0., index=(1+np.arange(9))/10, columns=['Accuracy'])\n",
    "for i, frac in enumerate((1+np.arange(9))/10):\n",
    "    sim_cls = pd.read_csv('./synthetic_data/sim_cls_high.csv', index_col=1)\n",
    "    z = np.loadtxt('./synth_results/embeddings_frac_{}.txt'.format(frac))\n",
    "    labels = sim_cls['cluster.id'].to_numpy() - 1\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "    acc_temp = 0.\n",
    "    for train_idx, test_idx in kf.split(z):\n",
    "        X_train, X_test = z[train_idx], z[test_idx]\n",
    "        y_train, y_test = labels[train_idx], labels[test_idx]\n",
    "    \n",
    "        model = xgb.XGBClassifier(\n",
    "            objective='multi:softmax',  # or 'multi:softprob' if you want probabilities\n",
    "            num_class=len(np.unique(labels)),\n",
    "            eval_metric='mlogloss'\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict classes\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc_temp += np.mean(y_pred == y_test)\n",
    "    my_res_ACC.loc[frac, 'Accuracy'] = acc_temp / 10.\n",
    "print(my_res_ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2195254e-b79c-4e0d-b932-0c850dd446f2",
   "metadata": {},
   "source": [
    "## Downstreaming task 3: Reconstruction\n",
    "Predict the masked portion of the synthetic data from the output embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a77e9a6c-025b-4d2c-aab0-8d823d77cf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Averaged Correlation\n",
      "0.1              0.695623\n",
      "0.2              0.722080\n",
      "0.3              0.736973\n",
      "0.4              0.750532\n",
      "0.5              0.763948\n",
      "0.6              0.766739\n",
      "0.7              0.782772\n",
      "0.8              0.775191\n",
      "0.9              0.802561\n"
     ]
    }
   ],
   "source": [
    "ans = pd.DataFrame(0., columns=['Averaged Correlation'], index=np.array(range(1, 10))/10.)\n",
    "for i, frac in enumerate(np.array(range(1, 10))/10.):\n",
    "    anss = []\n",
    "\n",
    "    RNA_pred = np.loadtxt('./synth_results/RNA_expr_missing_pred_high_noise_frac_{}.txt'.format(frac))\n",
    "    RNA_obs = np.loadtxt('./synth_results/RNA_expr_missing_obs_high_noise_frac_{}.txt'.format(frac))\n",
    "    anss += [np.corrcoef(RNA_pred, RNA_obs)[1,0]]\n",
    "\n",
    "    protein_pred = np.loadtxt('./synth_results/Protein_missing_pred_high_noise_frac_{}.txt'.format(frac))\n",
    "    protein_obs = np.loadtxt('./synth_results/Protein_missing_obs_high_noise_frac_{}.txt'.format(frac))\n",
    "    anss += [np.corrcoef(protein_pred, protein_obs)[1,0]]\n",
    "\n",
    "    DNA_pred = np.loadtxt('./synth_results/DNA_methyl_missing_obs_high_noise_frac_{}.txt'.format(frac))\n",
    "    DNA_obs = np.loadtxt('./synth_results/DNA_methyl_missing_pred_high_noise_frac_{}.txt'.format(frac))\n",
    "    anss += [np.corrcoef(DNA_pred, DNA_obs)[1,0]]\n",
    "\n",
    "    ans.loc[frac] = sum(anss)/3.\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd996818-b287-4b1e-bf51-d22aca977905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
